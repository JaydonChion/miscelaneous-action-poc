{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys  \n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "data_path=\"npz_data/violence/*\"\n",
    "# data_path2=\"npz_data/testtalk/*\"\n",
    "\n",
    "img_size=(224,224,3)\n",
    "frame_number=20\n",
    "num_class=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HEIGHT = 28\n",
    "# WIDTH = 28\n",
    "# DEPTH = 3\n",
    "\n",
    "# NUM_PER_EPOCH = 50000\n",
    "\n",
    "# def parser(serialized_example):\n",
    "#     \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "#     features = tf.parse_single_example(\n",
    "#         serialized_example,\n",
    "#         features={\n",
    "#             'image': tf.FixedLenFeature([], tf.string),\n",
    "#             'label': tf.FixedLenFeature([], tf.string),\n",
    "#         })\n",
    "#     image = tf.decode_raw(features['image'], tf.uint8)\n",
    "#     image.set_shape([924*20*DEPTH * HEIGHT * WIDTH])\n",
    "\n",
    "#     # Reshape from [depth * height * width] to [depth, height, width].\n",
    "#     image = tf.cast(\n",
    "#         tf.transpose(tf.reshape(image, [-1,20, HEIGHT, WIDTH,DEPTH])),\n",
    "#         tf.float32)\n",
    "#     label = tf.cast(features['label'], tf.float32)\n",
    "\n",
    "#     return image, label\n",
    "\n",
    "\n",
    "\n",
    "# def make_batch(batch_size):\n",
    "#     \"\"\"Read the images and labels from 'filenames'.\"\"\"\n",
    "#     filenames = [os.path.join(\"./npz_data/violence\", 'train.tfrecords')]\n",
    "\n",
    "#     # Repeat infinitely.\n",
    "#     dataset = tf.data.TFRecordDataset(filenames).repeat()\n",
    "\n",
    "#     # Parse records.\n",
    "#     dataset = dataset.map(\n",
    "#         parser)\n",
    "\n",
    "#     # Potentially shuffle records.\n",
    "#     min_queue_examples = int(NUM_PER_EPOCH * 0.4)\n",
    "#     dataset = dataset.shuffle(buffer_size=min_queue_examples + 3 * batch_size)\n",
    "\n",
    "#     # Batch it up.\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     iterator = dataset.make_one_shot_iterator()\n",
    "#     image_batch, label_batch = iterator.get_next()\n",
    "\n",
    "#     return image_batch, label_batch\n",
    "\n",
    "# sess =tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# data,label = sess.run(make_batch(5))\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _parse_function(example_proto):\n",
    "#     features = {\"data\": tf.FixedLenFeature((), tf.float32),\n",
    "#                 \"label\": tf.FixedLenFeature((), tf.float32)} \n",
    "#     parsed_features = tf.parse_single_example(example_proto, features) \n",
    "# #     data = tf.decode_raw(parsed_features['data'], tf.float32)\n",
    "#     return parsed_features[\"data\"], parsed_features[\"label\"]\n",
    "# def load_tfrecords(srcfile): \n",
    "#     sess = tf.Session()\n",
    "#     dataset = tf.data.TFRecordDataset(srcfile) # load tfrecord file dataset =\n",
    "#     dataset.map(_parse_function) # parse data into tensor dataset = \n",
    "#     dataset.repeat(30) # repeat for 2 epoches dataset =\n",
    "#     dataset.batch(5) # set batch_size = 5 iterator = \n",
    "#     iterator = dataset.make_one_shot_iterator()\n",
    "#     next_data = iterator.get_next()\n",
    "#     while True:\n",
    "#         try: \n",
    "#             data= sess.run(next_data) \n",
    "#             print (data.data)\n",
    "# #             print (label) \n",
    "#         except tf.errors.OutOfRangeError: \n",
    "#             break\n",
    "\n",
    "            \n",
    "            \n",
    "# load_tfrecords(srcfile=\"npz_data/violence/train.tfrecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TFRecordDatasetV1 shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "def _parse_function(example_proto):\n",
    "    features = {\"data\": tf.FixedLenFeature((), tf.float32),\n",
    "                \"label\": tf.FixedLenFeature((), tf.float32)} \n",
    "    parsed_features = tf.parse_single_example(example_proto, features) \n",
    "\n",
    "#     data = tf.decode_raw(parsed_features['data'], tf.float32)\n",
    "    return parsed_features[\"data\"], parsed_features[\"label\"]\n",
    "def load_tfrecords(srcfile): \n",
    "    sess = tf.Session()\n",
    "    dataset = tf.data.TFRecordDataset(srcfile) # load tfrecord file dataset =\n",
    "    dataset.map(_parse_function) # parse data into tensor dataset = \n",
    "    dataset.repeat(30) # repeat for 2 epoches dataset =\n",
    "    dataset.batch(5) # set batch_size = 5 iterator = \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "#     iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    return next_element,iterator\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    nextData, myIter = load_tfrecords(srcfile=\"npz_data/violence/train.tfrecords\")\n",
    "\n",
    "    sess.run(myIter.initializer)\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        mydata = sess.run(nextData.numpy())\n",
    "        print(mydata.shape)\n",
    "#             print(sess.run((next_data,next_label)))\n",
    "\n",
    "    except:\n",
    "    # Raised when we reach the end of the file.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with tf.Session() as sess:\n",
    "#     try:\n",
    "#         while True:\n",
    "#             print(\"reading\")\n",
    "#             data_record = sess.run(iterator.get_next())\n",
    "#             print(data_record)\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    data=np.empty([1,frame_number,img_size[0],img_size[1],3])\n",
    "    label=np.empty([1,num_class])\n",
    "    file_list=glob.glob(data_path)\n",
    "    data_list=[d for d in file_list if \"data\" in d.split(\"/\")[-1]]\n",
    "    label_list=[l for l in file_list if \"label\" in l.split(\"/\")[-1]]\n",
    "#     for index in range(len(data_list)):\n",
    "#         d=np.load(data_list[index])[\"arr_0\"]\n",
    "#         l=np.load(label_list[index])[\"arr_0\"]\n",
    "#         data=np.append(data,d,axis=0)\n",
    "#         data=np.append(label,l,axis=0)\n",
    "#     data=data[1:]\n",
    "#     label=label[1:]\n",
    "    for index in range(len(data_list)):\n",
    "        data=np.load(data_list[index])[\"arr_0\"]\n",
    "        label=np.load(label_list[index])[\"arr_0\"]\n",
    "    return data,label\n",
    "\n",
    "data,label=load_data(data_path)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "#try to upsize the data\n",
    "sample=np.empty([len(data),frame_number,img_size[0],img_size[1],3])\n",
    "\n",
    "print(sample.shape)\n",
    "\n",
    "for frameCubeNum in range(len(data)):\n",
    "    frameCube = data[frameCubeNum,...]\n",
    "    frame_stack=np.empty([frame_number,img_size[0],img_size[1],3])\n",
    "#     print(\"size of frame cube {}\".format(frame_stack.shape))\n",
    "\n",
    "    for imgcubeNum in range(len(frameCube)):\n",
    "        imgcube = frameCube[imgcubeNum,...]\n",
    "        img_stack=np.empty([img_size[0],img_size[1],3])\n",
    "#         print(\"size of imgcube cube {}\".format(img_stack.shape))\n",
    "\n",
    "        for imglayerNum in range(3):\n",
    "            imglayer = imgcube[...,imglayerNum]\n",
    "            img_sm = cv2.resize(imglayer, (img_size[0],img_size[1]), interpolation=cv2.INTER_CUBIC)\n",
    "#             print(\"size of imglayer {}\".format(imglayer.shape))\n",
    "            img_stack[...,imglayerNum] = img_sm\n",
    "        \n",
    "        frame_stack[imgcubeNum,...]=img_stack\n",
    "#         frame_stack=np.append(frame_stack,img_stack,axis=0)\n",
    "#         print(frameCube.shape)\n",
    "        \n",
    "    sample[frameCubeNum,...]=frame_stack\n",
    "    \n",
    "    \n",
    "    \n",
    "data = sample\n",
    "\n",
    "print(data.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load extra test data\n",
    "# def load_test_data(data_path):\n",
    "#     data=np.empty([1,frame_number,img_size[0],img_size[1],3])\n",
    "#     label=np.empty([1,num_class])\n",
    "#     file_list=glob.glob(data_path)\n",
    "#     data_list=[d for d in file_list if \"data\" in d.split(\"/\")[-1]]\n",
    "#     label_list=[l for l in file_list if \"label\" in l.split(\"/\")[-1]]\n",
    "# #     for index in range(len(data_list)):\n",
    "# #         d=np.load(data_list[index])[\"arr_0\"]\n",
    "# #         l=np.load(label_list[index])[\"arr_0\"]\n",
    "# #         data=np.append(data,d,axis=0)\n",
    "# #         data=np.append(label,l,axis=0)\n",
    "# #     data=data[1:]\n",
    "# #     label=label[1:]\n",
    "#     for index in range(len(data_list)):\n",
    "#         data=np.load(data_list[index])[\"arr_0\"]\n",
    "#         label=np.load(label_list[index])[\"arr_0\"]\n",
    "#     return data,label\n",
    "\n",
    "# test2data,test2label=load_test_data(data_path2)\n",
    "\n",
    "# print(test2data.shape)\n",
    "\n",
    "# #try to upsize the data\n",
    "# sample2=np.empty([len(test2data),frame_number,img_size[0],img_size[1],3])\n",
    "\n",
    "# print(sample2.shape)\n",
    "\n",
    "# for frameCubeNum in range(len(test2data)):\n",
    "#     frameCube = test2data[frameCubeNum,...]\n",
    "#     frame_stack=np.empty([frame_number,img_size[0],img_size[1],3])\n",
    "# #     print(\"size of frame cube {}\".format(frame_stack.shape))\n",
    "\n",
    "#     for imgcubeNum in range(len(frameCube)):\n",
    "#         imgcube = frameCube[imgcubeNum,...]\n",
    "#         img_stack=np.empty([img_size[0],img_size[1],3])\n",
    "# #         print(\"size of imgcube cube {}\".format(img_stack.shape))\n",
    "\n",
    "#         for imglayerNum in range(3):\n",
    "#             imglayer = imgcube[...,imglayerNum]\n",
    "#             img_sm = cv2.resize(imglayer, (img_size[0],img_size[1]), interpolation=cv2.INTER_CUBIC)\n",
    "# #             print(\"size of imglayer {}\".format(imglayer.shape))\n",
    "#             img_stack[...,imglayerNum] = img_sm\n",
    "        \n",
    "#         frame_stack[imgcubeNum,...]=img_stack\n",
    "# #         frame_stack=np.append(frame_stack,img_stack,axis=0)\n",
    "# #         print(frameCube.shape)\n",
    "        \n",
    "#     sample2[frameCubeNum,...]=frame_stack\n",
    "    \n",
    "    \n",
    "    \n",
    "# test2data = sample2\n",
    "\n",
    "# print(test2data.shape)\n",
    "# print(test2label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = data[:int(label.shape[0]*0.8),...]\n",
    "# train_y = label[:int(label.shape[0]*0.8),...]\n",
    "# valid_x = data[int(label.shape[0]*0.8):int(label.shape[0]*0.9),...]\n",
    "# valid_y = label[int(label.shape[0]*0.8):int(label.shape[0]*0.9),...]\n",
    "# test_x = data[int(label.shape[0]*0.9):,...]\n",
    "# test_y = label[int(label.shape[0]*0.9):,...]\n",
    "\n",
    "train_x, valid_x, train_y,valid_y =  train_test_split(data, label, test_size=0.3,random_state=42)\n",
    "test_x, valid_x, test_y,valid_y =  train_test_split(valid_x, valid_y, test_size=0.5, random_state=42)\n",
    "\n",
    "del data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-6\n",
    "X = tf.placeholder(tf.float32,[None,frame_number,224,224,3])\n",
    "labels = tf.placeholder(tf.float32,[None,num_class])\n",
    "\n",
    "\n",
    "module = hub.Module(\"https://tfhub.dev/deepmind/i3d-kinetics-600/1\",trainable=False)\n",
    "outputs = module(X)\n",
    "\n",
    "dense1 = tf.layers.dense(inputs=outputs, units=128)\n",
    "dropped1 = tf.nn.dropout(dense1,keep_prob=0.5)\n",
    "dense2 = tf.layers.dense(inputs=dropped1, units=128)\n",
    "dropped2 = tf.nn.dropout(dense2,keep_prob=0.5)\n",
    "# dense2 = tf.layers.dense(inputs=dense1, units=128)\n",
    "# dense3 = tf.layers.dense(inputs=dense2, units=128)\n",
    "\n",
    "logits = tf.layers.dense(inputs=dropped2, units=num_class)\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "        \n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_steps = int(train_x.shape[0]/batch_size)\n",
    "display_step = 2 #was 2 when achieved test2 accuracy 0.667\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(10):   #was 10\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "\n",
    "        for step in range(0, num_steps):\n",
    "            batch_x, batch_y = train_x[step:(step+1)*batch_size,...],train_y[step:(step+1)*batch_size,...]\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, labels: batch_y})\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                    # Calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                         labels: batch_y})\n",
    "                print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "\n",
    "        print(\"Optimization Finished fpr epoch {}\\n\".format(epoch))\n",
    "        valaccuracy =sess.run(accuracy, feed_dict={X: valid_x,\n",
    "                                              labels:valid_y});\n",
    "        print(\"validation Accuracy: {}\".format(valaccuracy))\n",
    "            \n",
    "        f = open(\"result.txt\",\"a+\")\n",
    "        f.write(\"validation accuracy for epoch {} is {}\\n\".format(epoch,valaccuracy))\n",
    "        f.close()\n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "\n",
    "    del train_x,train_y,valid_x,valid_y\n",
    "    testaccuracy = sess.run(accuracy, feed_dict={X: test_x,\n",
    "                                              labels:test_y})\n",
    "    print(\"Testing accuracy is {} \\n\".format(testaccuracy))\n",
    "    f = open(\"result.txt\",\"a+\")\n",
    "    f.write(\"testing accuracy is {}\\n\".format(testaccuracy))\n",
    "    f.close()\n",
    "    \n",
    "    del test_x,test_y\n",
    "#     print(test2label)\n",
    "#     testaccuracy2 = sess.run(accuracy, feed_dict={X: test2data,\n",
    "#                                               labels:test2label})\n",
    "#     print(\"Testing2 accuracy is {} \\n\".format(testaccuracy2))\n",
    "#     f = open(\"result.txt\",\"a+\")\n",
    "#     f.write(\"testing2 accuracy is {}\\n\".format(testaccuracy2))\n",
    "#     f.close()\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     saver = tf.train.import_meta_graph('/tmp/model.ckpt.meta')\n",
    "#     saver.restore(sess, \"/tmp/model.ckpt\")    \n",
    "#     print(\"Model restored.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(5)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Typically `result` will be the output of a model, or an optimizer's\n",
    "# training operation.\n",
    "result = tf.add(next_element, next_element)\n",
    "sess = tf.Session()\n",
    "sess.run(iterator.initializer)\n",
    "print(sess.run(result))  # ==> \"0\"\n",
    "print(sess.run(result))  # ==> \"2\"\n",
    "print(sess.run(result))  # ==> \"4\"\n",
    "print(sess.run(result))  # ==> \"6\"\n",
    "print(sess.run(result))  # ==> \"8\"\n",
    "try:\n",
    "    sess.run(result)\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print(\"End of dataset\")  # ==> \"End of dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
